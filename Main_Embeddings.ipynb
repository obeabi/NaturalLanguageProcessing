{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Main_Embeddings.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPvO8DOQJPmfqA/ws61PVSF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/obeabi/NaturalLanguageProcessing/blob/master/Main_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzR_fZSXnFpn"
      },
      "source": [
        "## A tutorial on Word Embeddings\r\n",
        "## Date 2020-12-21\r\n",
        "## Objective: Experiemnt with Word Embdedding models in lieterature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0q0c0ZP4nRcs",
        "outputId": "92be3837-ba82-4895-a186-1c9cadc78de7"
      },
      "source": [
        "# install libraries\r\n",
        "!pip install nltk\r\n",
        "!pip install gensim\r\n",
        "!pip install\r\n",
        "!pip install fasttext\r\n",
        "!pip install spacy\r\n",
        "print(\"installation complete\")\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.19.4)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (4.0.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\n",
            "Collecting glove\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/c9/17c400d0c29746162bd47fc719bf3212b2b031949d41d712e9bdef11ae03/glove-1.0.2.tar.gz (44kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from glove) (1.19.4)\n",
            "Building wheels for collected packages: glove\n",
            "  Building wheel for glove (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for glove\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for glove\n",
            "Failed to build glove\n",
            "Installing collected packages: glove\n",
            "    Running setup.py install for glove ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-3bzm8tai/glove/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-3bzm8tai/glove/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-9fvz5tr5/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n",
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (50.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.19.4)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3040253 sha256=fcbe913e4f4f296765638b262586e5fc79b56d82b2535c7d2d79473e5428d7bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.19.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n",
            "installation complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnB-gISepWrB",
        "outputId": "79b954bd-b443-4f3b-b7eb-def42fc8f269"
      },
      "source": [
        "# Python program to generate word vectors using Word2Vec \r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "# importing all necessary modules \r\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize \r\n",
        "import warnings \r\n",
        "  \r\n",
        "warnings.filterwarnings(action = 'ignore') \r\n",
        "  \r\n",
        "import gensim \r\n",
        "from gensim.models import Word2Vec \r\n",
        "\r\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amn8cYLZpqI2",
        "outputId": "732ff38c-e962-44f0-8a8c-cf34f0cd67fe"
      },
      "source": [
        "#  Reads ‘text.txt’ file \r\n",
        "sample = open(\"text.txt\", \"r\") \r\n",
        "s = sample.read() \r\n",
        "\r\n",
        "# Replaces escape character with space \r\n",
        "f = s.replace(\"\\n\", \" \") \r\n",
        "data = [] \r\n",
        "  \r\n",
        "# iterate through each sentence in the file \r\n",
        "for i in sent_tokenize(f): \r\n",
        "    temp = [] \r\n",
        "      \r\n",
        "    # tokenize the sentence into words \r\n",
        "    for j in word_tokenize(i): \r\n",
        "        temp.append(j.lower()) \r\n",
        "  \r\n",
        "    data.append(temp) \r\n",
        "data[0:2]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['the',\n",
              "  'project',\n",
              "  'gutenberg',\n",
              "  'ebook',\n",
              "  'of',\n",
              "  'alice',\n",
              "  '’',\n",
              "  's',\n",
              "  'adventures',\n",
              "  'in',\n",
              "  'wonderland',\n",
              "  ',',\n",
              "  'by',\n",
              "  'lewis',\n",
              "  'carroll',\n",
              "  'this',\n",
              "  'ebook',\n",
              "  'is',\n",
              "  'for',\n",
              "  'the',\n",
              "  'use',\n",
              "  'of',\n",
              "  'anyone',\n",
              "  'anywhere',\n",
              "  'in',\n",
              "  'the',\n",
              "  'united',\n",
              "  'states',\n",
              "  'and',\n",
              "  'most',\n",
              "  'other',\n",
              "  'parts',\n",
              "  'of',\n",
              "  'the',\n",
              "  'world',\n",
              "  'at',\n",
              "  'no',\n",
              "  'cost',\n",
              "  'and',\n",
              "  'with',\n",
              "  'almost',\n",
              "  'no',\n",
              "  'restrictions',\n",
              "  'whatsoever',\n",
              "  '.'],\n",
              " ['you',\n",
              "  'may',\n",
              "  'copy',\n",
              "  'it',\n",
              "  ',',\n",
              "  'give',\n",
              "  'it',\n",
              "  'away',\n",
              "  'or',\n",
              "  're-use',\n",
              "  'it',\n",
              "  'under',\n",
              "  'the',\n",
              "  'terms',\n",
              "  'of',\n",
              "  'the',\n",
              "  'project',\n",
              "  'gutenberg',\n",
              "  'license',\n",
              "  'included',\n",
              "  'with',\n",
              "  'this',\n",
              "  'ebook',\n",
              "  'or',\n",
              "  'online',\n",
              "  'at',\n",
              "  'www.gutenberg.org',\n",
              "  '.']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N40cxLuHqHhi"
      },
      "source": [
        "# Create CBOW model \r\n",
        "model1 = gensim.models.Word2Vec(data, min_count = 1,  \r\n",
        "                              size = 100, window = 5) \r\n",
        "model1.save('CBOW_word_2_vec.model')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDAm97iqsgr9",
        "outputId": "b9c3adc7-fbee-420c-88c6-d685fa7a5e19"
      },
      "source": [
        "# Print results \r\n",
        "print(\"Cosine similarity between 'alice' \" + \r\n",
        "               \"and 'wonderland' - CBOW : \", \r\n",
        "    model1.similarity('alice', 'wonderland')) \r\n",
        "      \r\n",
        "print(\"Cosine similarity between 'alice' \" +\r\n",
        "                 \"and 'machines' - CBOW : \", \r\n",
        "      model1.similarity('alice', 'machines')) \r\n",
        "\r\n",
        "      \r\n",
        "print(\"Cosine similarity between 'alice' \" +\r\n",
        "                 \"and 'canada' - CBOW : \", \r\n",
        "      model1.similarity('alice', 'apples')) \r\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cosine similarity between 'alice' and 'wonderland' - CBOW :  0.99943733\n",
            "Cosine similarity between 'alice' and 'machines' - CBOW :  0.9706878\n",
            "Cosine similarity between 'alice' and 'canada' - CBOW :  0.98707664\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SECS_gLtGW1"
      },
      "source": [
        "# Create Skip Gram model \r\n",
        "model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100, \r\n",
        "                                             window = 5, sg = 1)\r\n",
        "\r\n",
        "model2.save(\"SkipGram_Word_2_vec.model\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NB-tDc2tPzr",
        "outputId": "08d44240-265b-4b3b-cdee-799da8cc39d4"
      },
      "source": [
        "# Print results \r\n",
        "print(\"Cosine similarity between 'alice' \" + \r\n",
        "               \"and 'wonderland' - CBOW : \", \r\n",
        "    model2.similarity('alice', 'wonderland')) \r\n",
        "      \r\n",
        "print(\"Cosine similarity between 'alice' \" +\r\n",
        "                 \"and 'machines' - CBOW : \", \r\n",
        "      model2.similarity('alice', 'machines')) \r\n",
        "\r\n",
        "      \r\n",
        "print(\"Cosine similarity between 'alice' \" +\r\n",
        "                 \"and 'canada' - CBOW : \", \r\n",
        "      model2.similarity('alice', 'apples')) "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cosine similarity between 'alice' and 'wonderland' - CBOW :  0.8709415\n",
            "Cosine similarity between 'alice' and 'machines' - CBOW :  0.87572914\n",
            "Cosine similarity between 'alice' and 'canada' - CBOW :  0.8735428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbzT62rXyIga"
      },
      "source": [
        "### Spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "WhdmQn_0yKdI",
        "outputId": "0bc78e32-e169-4e22-c581-eb7e28c0b53e"
      },
      "source": [
        "import spacy \r\n",
        "fro\r\n",
        "  \r\n",
        "nlp = spacy.load('en_core_web_md') \r\n",
        "  \r\n",
        "print(\"Enter two space-separated words\") \r\n",
        "words = input() \r\n",
        "  \r\n",
        "tokens = nlp(words) \r\n",
        "  \r\n",
        "for token in tokens: \r\n",
        "    # Printing the following attributes of each token. \r\n",
        "    # text: the word string, has_vector: if it contains \r\n",
        "    # a vector representation in the model,  \r\n",
        "    # vector_norm: the algebraic norm of the vector, \r\n",
        "    # is_oov: if the word is out of vocabulary. \r\n",
        "    print(token.text, token.has_vector, token.vector_norm, token.is_oov) \r\n",
        "  \r\n",
        "token1, token2 = tokens[0], tokens[1] \r\n",
        "  \r\n",
        "print(\"Similarity:\", token1.similarity(token2))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-ed11d5de6a2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_md'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter two space-separated words\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_md'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
          ]
        }
      ]
    }
  ]
}